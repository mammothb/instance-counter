{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\mrcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\mrcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\mrcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\mrcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\mrcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\mrcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os.path\r\n",
    "import sys\r\n",
    "\r\n",
    "import cv2\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "from PIL import Image\r\n",
    "from six.moves import urllib\r\n",
    "\r\n",
    "ROOT_DIR = os.path.abspath(\"..\")\r\n",
    "sys.path.append(ROOT_DIR)\r\n",
    "\r\n",
    "from src.deeplab.model import DeepLabModel\r\n",
    "import src.deeplab.visualize as dl_viz\r\n",
    "from src.mrcnn import utils\r\n",
    "import src.mrcnn.coco as coco\r\n",
    "from src.mrcnn.model import MaskRCNN\r\n",
    "import src.mrcnn.visualize as mrcnn_viz\r\n",
    "\r\n",
    "\r\n",
    "%matplotlib inline\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepLab model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\r\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"weights\")\r\n",
    "\r\n",
    "# DeepLab\r\n",
    "DL_MODEL_PATH = os.path.join(MODEL_DIR, \"deeplab_model.tar.gz\")\r\n",
    "DL_URL = (\r\n",
    "    \"http://download.tensorflow.org/models/\"\r\n",
    "    \"deeplabv3_mnv2_ade20k_train_2018_12_03.tar.gz\"\r\n",
    ")\r\n",
    "\r\n",
    "if not os.path.exists(DL_MODEL_PATH):\r\n",
    "    print(\"Downloading DeepLab model...\")\r\n",
    "    urllib.request.urlretrieve(DL_URL, DL_MODEL_PATH)\r\n",
    "    print(\"Download complete, loading DeepLab model...\")\r\n",
    "\r\n",
    "DL_MODEL = DeepLabModel(DL_MODEL_PATH)\r\n",
    "print(\"DeepLab model loaded successfully!\")\r\n",
    "\r\n",
    "# MRCNN\r\n",
    "class InferenceConfig(coco.CocoConfig):\r\n",
    "    # Set batch size to 1 since we'll be running inference on\r\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\r\n",
    "    GPU_COUNT = 1\r\n",
    "    IMAGES_PER_GPU = 1\r\n",
    "\r\n",
    "MRCNN_MODEL_PATH = os.path.join(MODEL_DIR, \"mask_rcnn_coco.h5\")\r\n",
    "# Download COCO trained weights from Releases if needed\r\n",
    "if not os.path.exists(MRCNN_MODEL_PATH):\r\n",
    "    utils.download_trained_weights(MRCNN_MODEL_PATH)\r\n",
    "config = InferenceConfig()\r\n",
    "# Create model object in inference mode.\r\n",
    "MRCNN_MODEL = MaskRCNN(mode=\"inference\", model_dir=LOGS_DIR, config=config)\r\n",
    "\r\n",
    "# Load weights trained on MS-COCO\r\n",
    "MRCNN_MODEL.load_weights(MRCNN_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up labels and colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_LABEL_NAMES = np.asarray([\r\n",
    "    \"others\", \"wall\", \"building\", \"sky\", \"floor\", \"tree\", \"ceiling\", \"road\",\r\n",
    "    \"bed\", \"windowpane\", \"grass\", \"cabinet\", \"sidewalk\", \"person\", \"earth\",\r\n",
    "    \"door\", \"table\", \"mountain\", \"plant\", \"curtain\", \"chair\", \"car\", \"water\",\r\n",
    "    \"painting\", \"sofa\", \"shelf\", \"house\", \"sea\", \"mirror\", \"rug\", \"field\",\r\n",
    "    \"armchair\", \"seat\", \"fence\", \"desk\", \"rock\", \"wardrobe\", \"lamp\", \"bathtub\",\r\n",
    "    \"railing\", \"cushion\", \"base\", \"box\", \"column\", \"signboard\", \"chest\",\r\n",
    "    \"counter\", \"sand\", \"sink\", \"skyscraper\", \"fireplace\", \"refrigerator\",\r\n",
    "    \"grandstand\", \"path\", \"stairs\", \"runway\", \"case\", \"pool\", \"pillow\",\r\n",
    "    \"screen\", \"stairway\", \"river\", \"bridge\", \"bookcase\", \"blind\", \"coffee\",\r\n",
    "    \"toilet\", \"flower\", \"book\", \"hill\", \"bench\", \"countertop\", \"stove\", \"palm\",\r\n",
    "    \"kitchen\", \"computer\", \"swivel\", \"boat\", \"bar\", \"arcade\", \"hovel\", \"bus\",\r\n",
    "    \"towel\", \"light\", \"truck\", \"tower\", \"chandelier\", \"awning\", \"streetlight\",\r\n",
    "    \"booth\", \"television\", \"airplane\", \"dirt\", \"apparel\", \"pole\", \"land\",\r\n",
    "    \"bannister\", \"escalator\", \"ottoman\", \"bottle\", \"buffet\", \"poster\", \"stage\",\r\n",
    "    \"van\", \"ship\", \"fountain\", \"conveyer\", \"canopy\", \"washer\", \"plaything\",\r\n",
    "    \"swimming\", \"stool\", \"barrel\", \"basket\", \"waterfall\", \"tent\", \"bag\",\r\n",
    "    \"minibike\", \"cradle\", \"oven\", \"ball\", \"food\", \"step\", \"tank\", \"trade\",\r\n",
    "    \"microwave\", \"pot\", \"animal\", \"bicycle\", \"lake\", \"dishwasher\", \"screen\",\r\n",
    "    \"blanket\", \"sculpture\", \"hood\", \"sconce\", \"vase\", \"traffic\", \"tray\",\r\n",
    "    \"ashcan\", \"fan\", \"pier\", \"crt\", \"plate\", \"monitor\", \"bulletin\", \"shower\",\r\n",
    "    \"radiator\", \"glass\", \"clock\", \"flag\"\r\n",
    "])\r\n",
    "DL_LABEL_DICT = {\r\n",
    "    \"sand\": [\r\n",
    "        \"awning\", \"bridge\", \"ceiling\", \"curtain\", \"earth\", \"grass\", \"land\",\r\n",
    "        \"rock\", \"tree\", \"wall\"\r\n",
    "    ],\r\n",
    "    \"sea\": [\"boat\", \"ship\"]\r\n",
    "}\r\n",
    "DL_LABEL_MERGE = np.arange(len(DL_LABEL_NAMES))\r\n",
    "for key in DL_LABEL_DICT:\r\n",
    "    key_index = np.where(DL_LABEL_NAMES == key)\r\n",
    "    for label in DL_LABEL_DICT[key]:\r\n",
    "        DL_LABEL_MERGE[np.where(DL_LABEL_NAMES == label)] = key_index\r\n",
    "DL_LABEL_MAP = np.arange(len(DL_LABEL_NAMES)).reshape(len(DL_LABEL_NAMES), 1)\r\n",
    "DL_COLOR_MAP = dl_viz.label_to_color_image(DL_LABEL_MAP, DL_LABEL_MERGE)\r\n",
    "\r\n",
    "MRCNN_LABEL_NAMES = [\r\n",
    "    \"BG\", \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\",\r\n",
    "    \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\r\n",
    "    \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\",\r\n",
    "    \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\",\r\n",
    "    \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\",\r\n",
    "    \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\r\n",
    "    \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\r\n",
    "    \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\r\n",
    "    \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\r\n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\",\r\n",
    "    \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\",\r\n",
    "    \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\",\r\n",
    "    \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\",\r\n",
    "    \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\r\n",
    "    \"teddy bear\", \"hair drier\", \"toothbrush\"\r\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = os.path.join(ROOT_DIR, \"videos\", \"People_sample_2.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(VIDEO_PATH)\r\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\r\n",
    "output_path = f\"{os.path.splitext(VIDEO_PATH)[0]}_processed.avi\"\r\n",
    "\r\n",
    "vid_writer = cv2.VideoWriter(\r\n",
    "    output_path,\r\n",
    "    cv2.VideoWriter_fourcc(\"M\", \"J\", \"P\", \"G\"),\r\n",
    "    fps,\r\n",
    "    (\r\n",
    "        round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\r\n",
    "        round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\r\n",
    "    ),\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "Done processing !!!\n",
      "Output file is stored as d:\\GitRepos\\instance-counter\\videos\\People_sample_2_processed.avi\n"
     ]
    }
   ],
   "source": [
    "sec = 0\r\n",
    "while True:\r\n",
    "    print(sec)\r\n",
    "    cap.set(1, sec * fps)\r\n",
    "    # Get frame from the video\r\n",
    "    has_frame, frame = cap.read()\r\n",
    "\r\n",
    "    # Stop the program if reached end of video\r\n",
    "    if not has_frame:\r\n",
    "        print(\"Done processing !!!\")\r\n",
    "        print(f\"Output file is stored as {output_path}\")\r\n",
    "        break\r\n",
    "\r\n",
    "    _, seg_map = DL_MODEL.run(Image.fromarray(frame[:, :, ::-1]))\r\n",
    "    seg_map = cv2.resize(\r\n",
    "        seg_map, frame.shape[:-1][::-1], interpolation=cv2.INTER_NEAREST\r\n",
    "    )\r\n",
    "    frame_copy = dl_viz.display_segmentation(\r\n",
    "        frame, seg_map, DL_LABEL_NAMES, DL_LABEL_MERGE\r\n",
    "    )\r\n",
    "\r\n",
    "    results = MRCNN_MODEL.detect([frame[:, :, ::-1]])\r\n",
    "    r = results[0]\r\n",
    "    frame_copy = mrcnn_viz.display_instances(\r\n",
    "        frame_copy,\r\n",
    "        r[\"rois\"],\r\n",
    "        r[\"masks\"],\r\n",
    "        r[\"class_ids\"],\r\n",
    "        MRCNN_LABEL_NAMES,\r\n",
    "        r[\"scores\"],\r\n",
    "    )\r\n",
    "\r\n",
    "    # Write the frame with the detection boxes\r\n",
    "    vid_writer.write(frame_copy.astype(np.uint8))\r\n",
    "    sec += 1\r\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has_frame, frame = cap.read()\r\n",
    "# _, seg_map = DL_MODEL.run(Image.fromarray(frame[:, :, ::-1]))\r\n",
    "# seg_map = cv2.resize(\r\n",
    "#     seg_map, frame.shape[:-1][::-1], interpolation=cv2.INTER_NEAREST\r\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_segmentation_label(seg_map):\r\n",
    "#     \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\r\n",
    "#     unique_labels = np.unique(DL_LABEL_MERGE[seg_map])\r\n",
    "#     ax = plt.subplot(1, 1, 1)\r\n",
    "#     plt.imshow(\r\n",
    "#         DL_COLOR_MAP[unique_labels].astype(np.uint8),\r\n",
    "#         interpolation=\"nearest\",\r\n",
    "#     )\r\n",
    "#     ax.yaxis.tick_right()\r\n",
    "#     plt.yticks(range(len(unique_labels)), DL_LABEL_NAMES[unique_labels])\r\n",
    "#     plt.xticks([], [])\r\n",
    "#     ax.tick_params(width=0.0)\r\n",
    "#     plt.grid(\"off\")\r\n",
    "#     plt.show()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_segmentation_label(seg_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_copy = dl_viz.display_segmentation(\r\n",
    "#     frame, seg_map, DL_LABEL_NAMES, DL_LABEL_MERGE\r\n",
    "# )\r\n",
    "# _, ax = plt.subplots(1, figsize=(16, 16))\r\n",
    "# height, width = frame_copy.shape[:2]\r\n",
    "# ax.set_ylim(height + 10, -10)\r\n",
    "# ax.set_xlim(-10, width + 10)\r\n",
    "# ax.axis(\"off\")\r\n",
    "# ax.imshow(frame_copy[:, :, ::-1])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = MRCNN_MODEL.detect([frame[:, :, ::-1]])\r\n",
    "# r = results[0]\r\n",
    "# frame_copy = mrcnn_viz.display_instances(\r\n",
    "#     frame_copy,\r\n",
    "#     r[\"rois\"],\r\n",
    "#     r[\"masks\"],\r\n",
    "#     r[\"class_ids\"],\r\n",
    "#     MRCNN_LABEL_NAMES,\r\n",
    "#     r[\"scores\"],\r\n",
    "# )\r\n",
    "\r\n",
    "# _, ax = plt.subplots(1, figsize=(16, 16))\r\n",
    "# height, width = frame_copy.shape[:2]\r\n",
    "# ax.set_ylim(height + 10, -10)\r\n",
    "# ax.set_xlim(-10, width + 10)\r\n",
    "# ax.axis(\"off\")\r\n",
    "# ax.imshow(frame_copy[:, :, ::-1])\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('mrcnn': conda)",
   "name": "python3613jvsc74a57bd0a4401751e03e5eff8414c85ecabdf49b3fe7902825fae0aba35b82c2ac9d033f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}